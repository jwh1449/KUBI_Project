{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juwZ6QX9GSBa"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse, parse_qs, urlencode\n",
        "import time\n",
        "import re\n",
        "\n",
        "def get_page_content(url, retries=3):\n",
        "    \"\"\"지정된 URL의 HTML 내용을 가져옵니다.\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            time.sleep(1)  # 서버 부하 방지를 위한 1초 딜레이\n",
        "            return response.text\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"페이지 가져오기 오류 {url} (시도 {attempt+1}/{retries}): {e}\")\n",
        "            if attempt + 1 == retries:\n",
        "                return None\n",
        "            time.sleep(2)  # 재시도 전 2초 대기\n",
        "    return None\n",
        "\n",
        "def parse_notice_content(url):\n",
        "    \"\"\"공지사항 상세 페이지에서 본문 내용을 추출합니다.\"\"\"\n",
        "    print(f\"상세 페이지 크롤링: {url}\")\n",
        "    html_content = get_page_content(url)\n",
        "    if not html_content:\n",
        "        print(f\"상세 페이지 가져오기 실패: {url}\")\n",
        "        return \"\"\n",
        "\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    content_div = soup.find('div', id='bbs_ntt_cn_con')\n",
        "    if not content_div:\n",
        "        print(f\"본문 내용을 찾을 수 없습니다: {url}\")\n",
        "        return \"\"\n",
        "\n",
        "    content = ' '.join(content_div.get_text(strip=True).split())\n",
        "    return content if content else \"\"\n",
        "\n",
        "def parse_notices(html_content, base_url):\n",
        "    \"\"\"HTML에서 공지사항을 추출하고 상세 페이지 내용을 가져옵니다.\"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    notices = []\n",
        "\n",
        "    notice_table = soup.find('table', class_='bbs_default list')\n",
        "    if not notice_table:\n",
        "        print(\"공지사항 테이블을 찾을 수 없습니다.\")\n",
        "        return notices, False\n",
        "\n",
        "    tbody = notice_table.find('tbody', class_='tb')\n",
        "    if not tbody:\n",
        "        print(\"테이블 본문을 찾을 수 없습니다.\")\n",
        "        return notices, False\n",
        "\n",
        "    notice_rows = tbody.find_all('tr')\n",
        "    if not notice_rows:\n",
        "        print(\"공지사항 행을 찾을 수 없습니다.\")\n",
        "        return notices, False\n",
        "\n",
        "    for row in notice_rows:\n",
        "        columns = row.find_all('td')\n",
        "        if len(columns) < 7:\n",
        "            continue\n",
        "\n",
        "        number = columns[0].get_text(strip=True)\n",
        "        if number == '[공지]':\n",
        "            print(f\"[공지] 항목 제외: {columns[2].get_text(strip=True)}\")\n",
        "            continue\n",
        "\n",
        "        campus = columns[1].get_text(strip=True)\n",
        "        if campus == '춘천':\n",
        "            print(f\"춘천 캠퍼스 제외: {columns[2].get_text(strip=True)}\")\n",
        "            continue\n",
        "\n",
        "        notice = {}\n",
        "        notice['number'] = number\n",
        "        notice['campus'] = campus\n",
        "        notice['title'] = columns[2].get_text(strip=True)\n",
        "\n",
        "        title_cell = columns[2].find('a')\n",
        "        ntt_no = None\n",
        "        if title_cell:\n",
        "            href = title_cell.get('href', '')\n",
        "            match = re.search(r'nttNo=(\\d+)', href)\n",
        "            if match:\n",
        "                ntt_no = match.group(1)\n",
        "            onclick = title_cell.get('onclick', '')\n",
        "            match = re.search(r'goDetail\\((\\d+)\\)', onclick)\n",
        "            if match:\n",
        "                ntt_no = match.group(1)\n",
        "\n",
        "        if ntt_no:\n",
        "            notice['link'] = f\"{base_url}/www/selectBbsNttView.do?bbsNo=37&nttNo={ntt_no}\"\n",
        "        else:\n",
        "            notice['link'] = ''\n",
        "            print(f\"nttNo 추출 실패: 제목={notice['title']}\")\n",
        "\n",
        "        notice['author'] = columns[3].get_text(strip=True)\n",
        "        notice['date'] = columns[5].get_text(strip=True)\n",
        "        notice['views'] = columns[6].get_text(strip=True)\n",
        "        notice['content'] = parse_notice_content(notice['link']) if notice['link'] else \"\"\n",
        "\n",
        "        print(f\"\\n공지사항: {notice['title']}\")\n",
        "        print(f\"URL: {notice['link']}\")\n",
        "        if notice['content']:\n",
        "            preview = notice['content'][:200] + ('...' if len(notice['content']) > 200 else '')\n",
        "            print(f\"본문 내용: {preview}\")\n",
        "        else:\n",
        "            print(\"본문 내용: 없음\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        notices.append(notice)\n",
        "\n",
        "    next_page_exists = bool(soup.select_one('a.next'))\n",
        "    return notices, next_page_exists\n",
        "\n",
        "def get_total_pages(html_content):\n",
        "    \"\"\"총 페이지 수를 추출합니다.\"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    paging_div = soup.find('div', class_='paging')\n",
        "    if not paging_div:\n",
        "        return 475\n",
        "\n",
        "    page_links = paging_div.find_all('a')\n",
        "    page_numbers = []\n",
        "\n",
        "    for link in page_links:\n",
        "        href = link.get('href', '')\n",
        "        match = re.search(r'pageIndex=(\\d+)', href)\n",
        "        if match:\n",
        "            page_numbers.append(int(match.group(1)))\n",
        "        elif 'goPage' in href:\n",
        "            match = re.search(r'goPage\\((\\d+)\\)', href)\n",
        "            if match:\n",
        "                page_numbers.append(int(match.group(1)))\n",
        "\n",
        "    return max(page_numbers, default=475)\n",
        "\n",
        "def crawl_all_notices(start_url, estimated_total_items=4748, page_size=10):\n",
        "    \"\"\"모든 페이지의 공지사항과 본문 내용을 크롤링합니다.\"\"\"\n",
        "    all_notices = []\n",
        "    current_page = 1\n",
        "    url_components = urlparse(start_url)\n",
        "    base_url = f\"{url_components.scheme}://{url_components.netloc}\"\n",
        "    query_params = parse_qs(url_components.query)\n",
        "\n",
        "    max_pages = (estimated_total_items + page_size - 1) // page_size\n",
        "    print(f\"추정 최대 페이지 수: {max_pages}\")\n",
        "\n",
        "    while current_page <= max_pages:\n",
        "        print(f\"페이지 {current_page}/{max_pages} 크롤링 중...\")\n",
        "        query_params['pageIndex'] = [str(current_page)]\n",
        "        new_query = urlencode(query_params, doseq=True)\n",
        "        current_url = f\"{url_components.scheme}://{url_components.netloc}{url_components.path}?{new_query}\"\n",
        "\n",
        "        html_content = get_page_content(current_url)\n",
        "        if not html_content:\n",
        "            print(f\"페이지 {current_page} 가져오기 실패. 건너뜁니다.\")\n",
        "            current_page += 1\n",
        "            continue\n",
        "\n",
        "        notices, next_page_exists = parse_notices(html_content, base_url)\n",
        "        if not notices:\n",
        "            print(f\"페이지 {current_page}에서 공지사항이 없습니다. 크롤링 종료.\")\n",
        "            break\n",
        "\n",
        "        all_notices.extend(notices)\n",
        "        print(f\"페이지 {current_page}: {len(notices)}개 공지사항 수집 (총 {len(all_notices)}개, 다음 페이지 존재: {next_page_exists})\")\n",
        "\n",
        "        if current_page % 100 == 0:\n",
        "            print(f\"진행 상황: {current_page}/{max_pages} 페이지 완료\")\n",
        "            save_to_csv(all_notices, f'kangwon_notices_partial_{current_page}.csv')\n",
        "\n",
        "        if current_page == max_pages:\n",
        "            print(f\"마지막 페이지({current_page}) 도달. 공지사항 수: {len(notices)}, 다음 페이지 존재: {next_page_exists}\")\n",
        "\n",
        "        current_page += 1\n",
        "\n",
        "        if not next_page_exists and len(notices) < page_size:\n",
        "            print(\"다음 페이지가 없으며, 공지사항이 적게 수집되었습니다. 크롤링 종료.\")\n",
        "            break\n",
        "\n",
        "    return all_notices\n",
        "\n",
        "def save_to_csv(notices, filename='kangwon_notices.csv'):\n",
        "    \"\"\"공지사항을 CSV 파일로 저장합니다.\"\"\"\n",
        "    if not notices:\n",
        "        print(\"저장할 공지사항이 없습니다.\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(notices)\n",
        "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "    print(f\"{len(notices)}개의 공지사항을 {filename}에 저장했습니다.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_url = \"https://wwwk.kangwon.ac.kr/www/selectBbsNttList.do?bbsNo=37&key=1176&pageUnit=10&pageIndex=1\"\n",
        "    notices = crawl_all_notices(start_url)\n",
        "    save_to_csv(notices)"
      ]
    }
  ]
}